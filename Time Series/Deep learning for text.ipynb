{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep learning for text.ipynb","provenance":[],"authorship_tag":"ABX9TyMqC+RA0rvow4ugipvBC8Rh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Natural language processing\n","---\n","\n","In computer science, we refer to human languages, like English or Mandarin, as\n","“natural” languages, to distinguish them from languages that were designed for\n","machines, like Assembly, LISP, or XML. Every machine language was designed: its\n","starting point was a human engineer writing down a set of formal rules to describe what statements you could make in that language and what they meant. \n","\n","Rules came first, and people only started using the language once the rule set was complete. With human language, it’s the reverse: usage comes first, rules arise later.\n","\n","Creating algorithms that can make sense of natural language is a big deal. The internet is mostly text. Language is how we store almost all of our knowledge. Our very thoughts are largely built upon language. However, the ability to understand natural language has long eluded machines. \n","\n","Modern NLP is about using machine learning and large datasets to\n","give computers the ability not to understand language, but\n","to ingest a piece of language as input and return something useful, like predicting the following:\n","\n","- “What’s the topic of this text?” (text classification)\n","- “Does this text contain abuse?” (content filtering)\n","- “Does this text sound positive or negative?” (sentiment analysis)\n","- “What should be the next word in this incomplete sentence?” (language modeling)\n","- “How would you say this in German?” (translation)\n","- “How would you summarize this article in one paragraph?” (summarization)\n","\n","### Preparing text data\n","\n","Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. Vectorizing text is the process of transforming text into numeric tensors:\n","\n","- First, you standardize the text to make it easier to process, such as by converting it to lowercase or removing punctuation.\n","- You split the text into units (called tokens), such as characters, words, or groups of words. This is called tokenization.\n","- You convert each such token into a numerical vector. This will usually involve\n","first indexing all tokens present in the data.\n","\n","We are going to process the book Frankenstein, which is available from Project Gutenberg. [Here is a list of the most popular books on the site.](https://www.gutenberg.org/ebooks/search/%3Fsort_order%3Ddownloads)\n","\n"],"metadata":{"id":"2IUahgi8ObFF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Fk-7be65OICU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658766396428,"user_tz":300,"elapsed":660,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"02e30ef2-f9a0-4280-b40b-88d0f17529d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-25 16:26:34--  https://www.gutenberg.org/files/84/84-0.txt\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 448821 (438K) [text/plain]\n","Saving to: ‘84-0.txt’\n","\n","\r84-0.txt              0%[                    ]       0  --.-KB/s               \r84-0.txt            100%[===================>] 438.30K  --.-KB/s    in 0.07s   \n","\n","2022-07-25 16:26:34 (5.82 MB/s) - ‘84-0.txt’ saved [448821/448821]\n","\n"]}],"source":["!wget https://www.gutenberg.org/files/84/84-0.txt\n","!mv 84-0.txt frankenstein.txt"]},{"cell_type":"markdown","source":["Project Gutenberg adds a standard header and footer to each book and this is not part of the original text. We have to identify the header and footer, and remove them before processing the text."],"metadata":{"id":"VOi9z8It-7J_"}},{"cell_type":"code","source":["!head -30 frankenstein.txt\n","!echo \"-----------------\"\n","!tail -355 frankenstein.txt | head -10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSjucckod34v","executionInfo":{"status":"ok","timestamp":1658766576338,"user_tz":300,"elapsed":325,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"07a66721-0b99-4988-dcb7-e0ece1855eae"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["﻿The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\r\n","\r\n","This eBook is for the use of anyone anywhere in the United States and\r\n","most other parts of the world at no cost and with almost no restrictions\r\n","whatsoever. You may copy it, give it away or re-use it under the terms\r\n","of the Project Gutenberg License included with this eBook or online at\r\n","www.gutenberg.org. If you are not located in the United States, you\r\n","will have to check the laws of the country where you are located before\r\n","using this eBook.\r\n","\r\n","Title: Frankenstein\r\n","       or, The Modern Prometheus\r\n","\r\n","Author: Mary Wollstonecraft (Godwin) Shelley\r\n","\r\n","Release Date: 31, 1993 [eBook #84]\r\n","[Most recently updated: November 13, 2020]\r\n","\r\n","Language: English\r\n","\r\n","Character set encoding: UTF-8\r\n","\r\n","Produced by: Judith Boss, Christy Phillips, Lynn Hanninen, and David Meltzer. HTML version by Al Haines.\r\n","Further corrections by Menno de Leeuw.\r\n","\r\n","*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\r\n","\r\n","\r\n","\r\n","\r\n","-----------------\n","\n","\n","\n","*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\n","\n","***** This file should be named 84-0.txt or 84-0.zip *****\n","This and all associated files of various formats will be found in:\n","    https://www.gutenberg.org/8/84/\n","\n","Updated editions will replace the previous one--the old editions will\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","\n","from string import punctuation\n","\n","from tensorflow.keras import models, layers, optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"0f2lrzmIgBXO","executionInfo":{"status":"ok","timestamp":1658766592079,"user_tz":300,"elapsed":2524,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["lines = []\n","with open('frankenstein.txt') as f:\n","    lines = [line for line in f]\n","\n","lines = lines[30:-355]\n","\n","# We need to standardize the text, so we use letters in lower case, and remove punctuation signs.\n","raw_text = ''.join(lines).lower()\n","raw_text = raw_text.translate(str.maketrans(\"\", \"\", punctuation))\n","\n","print(raw_text[:5000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TWdg9w9gUye","executionInfo":{"status":"ok","timestamp":1658766874440,"user_tz":300,"elapsed":122,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"96d8ef11-0246-4c1e-93be-cbfa32f2a9f2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["frankenstein\n","\n","or the modern prometheus\n","\n","by mary wollstonecraft godwin shelley\n","\n","\n"," contents\n","\n"," letter 1\n"," letter 2\n"," letter 3\n"," letter 4\n"," chapter 1\n"," chapter 2\n"," chapter 3\n"," chapter 4\n"," chapter 5\n"," chapter 6\n"," chapter 7\n"," chapter 8\n"," chapter 9\n"," chapter 10\n"," chapter 11\n"," chapter 12\n"," chapter 13\n"," chapter 14\n"," chapter 15\n"," chapter 16\n"," chapter 17\n"," chapter 18\n"," chapter 19\n"," chapter 20\n"," chapter 21\n"," chapter 22\n"," chapter 23\n"," chapter 24\n","\n","\n","\n","\n","letter 1\n","\n","to mrs saville england\n","\n","\n","st petersburgh dec 11th 17—\n","\n","\n","you will rejoice to hear that no disaster has accompanied the\n","commencement of an enterprise which you have regarded with such evil\n","forebodings i arrived here yesterday and my first task is to assure\n","my dear sister of my welfare and increasing confidence in the success\n","of my undertaking\n","\n","i am already far north of london and as i walk in the streets of\n","petersburgh i feel a cold northern breeze play upon my cheeks which\n","braces my nerves and fills me with delight do you understand this\n","feeling this breeze which has travelled from the regions towards\n","which i am advancing gives me a foretaste of those icy climes\n","inspirited by this wind of promise my daydreams become more fervent\n","and vivid i try in vain to be persuaded that the pole is the seat of\n","frost and desolation it ever presents itself to my imagination as the\n","region of beauty and delight there margaret the sun is for ever\n","visible its broad disk just skirting the horizon and diffusing a\n","perpetual splendour there—for with your leave my sister i will put\n","some trust in preceding navigators—there snow and frost are banished\n","and sailing over a calm sea we may be wafted to a land surpassing in\n","wonders and in beauty every region hitherto discovered on the habitable\n","globe its productions and features may be without example as the\n","phenomena of the heavenly bodies undoubtedly are in those undiscovered\n","solitudes what may not be expected in a country of eternal light i\n","may there discover the wondrous power which attracts the needle and may\n","regulate a thousand celestial observations that require only this\n","voyage to render their seeming eccentricities consistent for ever i\n","shall satiate my ardent curiosity with the sight of a part of the world\n","never before visited and may tread a land never before imprinted by\n","the foot of man these are my enticements and they are sufficient to\n","conquer all fear of danger or death and to induce me to commence this\n","laborious voyage with the joy a child feels when he embarks in a little\n","boat with his holiday mates on an expedition of discovery up his\n","native river but supposing all these conjectures to be false you\n","cannot contest the inestimable benefit which i shall confer on all\n","mankind to the last generation by discovering a passage near the pole\n","to those countries to reach which at present so many months are\n","requisite or by ascertaining the secret of the magnet which if at\n","all possible can only be effected by an undertaking such as mine\n","\n","these reflections have dispelled the agitation with which i began my\n","letter and i feel my heart glow with an enthusiasm which elevates me\n","to heaven for nothing contributes so much to tranquillise the mind as\n","a steady purpose—a point on which the soul may fix its intellectual\n","eye this expedition has been the favourite dream of my early years i\n","have read with ardour the accounts of the various voyages which have\n","been made in the prospect of arriving at the north pacific ocean\n","through the seas which surround the pole you may remember that a\n","history of all the voyages made for purposes of discovery composed the\n","whole of our good uncle thomas’ library my education was neglected\n","yet i was passionately fond of reading these volumes were my study\n","day and night and my familiarity with them increased that regret which\n","i had felt as a child on learning that my father’s dying injunction\n","had forbidden my uncle to allow me to embark in a seafaring life\n","\n","these visions faded when i perused for the first time those poets\n","whose effusions entranced my soul and lifted it to heaven i also\n","became a poet and for one year lived in a paradise of my own creation\n","i imagined that i also might obtain a niche in the temple where the\n","names of homer and shakespeare are consecrated you are well\n","acquainted with my failure and how heavily i bore the disappointment\n","but just at that time i inherited the fortune of my cousin and my\n","thoughts were turned into the channel of their earlier bent\n","\n","six years have passed since i resolved on my present undertaking i\n","can even now remember the hour from which i dedicated myself to this\n","great enterprise i commenced by inuring my body to hardship i\n","accompanied the whalefishers on several expeditions to the north sea\n","i voluntarily endured cold famine thirst and want of sleep i often\n","worked harder than the common sailors during the day and devoted my\n","nights to the study of mathematics the theory of medicine and those\n","branches of physical science from which a naval adventurer might derive\n","the greatest practical advantage twice i actually h\n"]}]},{"cell_type":"markdown","source":["With the raw text in lower case, and without punctuation, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers."],"metadata":{"id":"mzgfA1KP_owf"}},{"cell_type":"code","source":["# Create a dictionary of characters to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","\n","print(char_to_int)\n","print(int_to_char)\n","\n","n_vocab = len(char_to_int)\n","n_chars = len(raw_text)\n","\n","print(f\"Number of distinct characters: {n_vocab}\\nTotal number of characters: {n_chars}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Z_nUaNYgd-1","executionInfo":{"status":"ok","timestamp":1658767574070,"user_tz":300,"elapsed":153,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"6625452b-9dcd-446e-eede-c44eb12d16bb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'\\n': 0, ' ': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37, 'æ': 38, 'è': 39, 'é': 40, 'ê': 41, 'ô': 42, '—': 43, '‘': 44, '’': 45, '“': 46, '”': 47}\n","{0: '\\n', 1: ' ', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z', 38: 'æ', 39: 'è', 40: 'é', 41: 'ê', 42: 'ô', 43: '—', 44: '‘', 45: '’', 46: '“', 47: '”'}\n","Number of distinct characters: 48\n","Total number of characters: 409737\n"]}]},{"cell_type":"markdown","source":["We need to create the sequences that are going to be fed to the neural network, as well as the \"labels\". In this case, the labels are the next character after a sequence."],"metadata":{"id":"AnX0kgeMCLC3"}},{"cell_type":"code","source":["seq_length = 100\n","\n","dataX = []\n","dataY = []\n","\n","for i in range(n_chars - seq_length):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n"," \n","n_patterns = len(dataX)"],"metadata":{"id":"vTKcFDwJkz22","executionInfo":{"status":"ok","timestamp":1658767735637,"user_tz":300,"elapsed":5136,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(dataX[0])\n","print(dataX[1])\n","print(dataY[0])\n","print(f\"Total pattersn: {n_patterns}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWQAm2UUliyt","executionInfo":{"status":"ok","timestamp":1658768017949,"user_tz":300,"elapsed":7,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"097c8569-0fd5-495e-8c7e-8b2b3c5991cf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[17, 29, 12, 25, 22, 16, 25, 30, 31, 16, 20, 25, 0, 0, 26, 29, 1, 31, 19, 16, 1, 24, 26, 15, 16, 29, 25, 1, 27, 29, 26, 24, 16, 31, 19, 16, 32, 30, 0, 0, 13, 36, 1, 24, 12, 29, 36, 1, 34, 26, 23, 23, 30, 31, 26, 25, 16, 14, 29, 12, 17, 31, 1, 18, 26, 15, 34, 20, 25, 1, 30, 19, 16, 23, 23, 16, 36, 0, 0, 0, 1, 14, 26, 25, 31, 16, 25, 31, 30, 0, 0, 1, 23, 16, 31, 31, 16, 29, 1, 3]\n","[29, 12, 25, 22, 16, 25, 30, 31, 16, 20, 25, 0, 0, 26, 29, 1, 31, 19, 16, 1, 24, 26, 15, 16, 29, 25, 1, 27, 29, 26, 24, 16, 31, 19, 16, 32, 30, 0, 0, 13, 36, 1, 24, 12, 29, 36, 1, 34, 26, 23, 23, 30, 31, 26, 25, 16, 14, 29, 12, 17, 31, 1, 18, 26, 15, 34, 20, 25, 1, 30, 19, 16, 23, 23, 16, 36, 0, 0, 0, 1, 14, 26, 25, 31, 16, 25, 31, 30, 0, 0, 1, 23, 16, 31, 31, 16, 29, 1, 3, 0]\n","0\n","Total pattersn: 409637\n"]}]},{"cell_type":"markdown","source":["Once we have the patters, we can transform them to numpy arrays, and perform simple normalization to each sequence."],"metadata":{"id":"vX9S18iUCbW1"}},{"cell_type":"code","source":["# reshape X to be [samples, time steps, features]\n","X = np.reshape(dataX, (n_patterns, seq_length, 1))\n","\n","# normalize\n","X = X / float(n_vocab)\n","\n","# one hot encode the output variable\n","y = to_categorical(dataY)"],"metadata":{"id":"VXkNh-PonQbu","executionInfo":{"status":"ok","timestamp":1658768263784,"user_tz":300,"elapsed":649,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(X.shape, y.shape)\n","print(y[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmwMh12_nrKk","executionInfo":{"status":"ok","timestamp":1658768297487,"user_tz":300,"elapsed":4,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"40f01cee-b964-4158-c332-e30a0ddc66cf"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["(409637, 100, 1) (409637, 48)\n","[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}]},{"cell_type":"code","source":["model = models.Sequential()\n","model.add(layers.LSTM(256, input_shape=(100, 1), return_sequences=True))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.LSTM(256))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Dense(48, activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=0.001), metrics=['accuracy'])\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTL7Yb4Tn6AE","executionInfo":{"status":"ok","timestamp":1658768431747,"user_tz":300,"elapsed":498,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"7e3115eb-1f73-4216-82e4-e4142e8dbf0a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_3 (LSTM)               (None, 100, 256)          264192    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 100, 256)          0         \n","                                                                 \n"," lstm_4 (LSTM)               (None, 256)               525312    \n","                                                                 \n"," dropout_4 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 48)                12336     \n","                                                                 \n","=================================================================\n","Total params: 801,840\n","Trainable params: 801,840\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["checkpoint = ModelCheckpoint('frankenstein_best_weights.h5', monitor='loss', verbose=1, save_best_only=True)\n","model.fit(X, y, epochs=20, batch_size=256, workers=8, callbacks=[checkpoint])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSLgEFJYojCF","executionInfo":{"status":"ok","timestamp":1658770352829,"user_tz":300,"elapsed":1825834,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"93b0f584-aa78-4064-a1a8-0b1b66ce5998"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 2.6638 - accuracy: 0.2304\n","Epoch 1: loss improved from inf to 2.66376, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 94s 54ms/step - loss: 2.6638 - accuracy: 0.2304\n","Epoch 2/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 2.3201 - accuracy: 0.3218\n","Epoch 2: loss improved from 2.66376 to 2.32009, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 2.3201 - accuracy: 0.3218\n","Epoch 3/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 2.1412 - accuracy: 0.3732\n","Epoch 3: loss improved from 2.32009 to 2.14119, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 88s 55ms/step - loss: 2.1412 - accuracy: 0.3732\n","Epoch 4/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 2.0263 - accuracy: 0.4057\n","Epoch 4: loss improved from 2.14119 to 2.02629, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 2.0263 - accuracy: 0.4057\n","Epoch 5/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.9438 - accuracy: 0.4286\n","Epoch 5: loss improved from 2.02629 to 1.94381, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.9438 - accuracy: 0.4286\n","Epoch 6/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.8788 - accuracy: 0.4469\n","Epoch 6: loss improved from 1.94381 to 1.87877, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.8788 - accuracy: 0.4469\n","Epoch 7/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.8269 - accuracy: 0.4618\n","Epoch 7: loss improved from 1.87877 to 1.82687, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 88s 55ms/step - loss: 1.8269 - accuracy: 0.4618\n","Epoch 8/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.7855 - accuracy: 0.4731\n","Epoch 8: loss improved from 1.82687 to 1.78550, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.7855 - accuracy: 0.4731\n","Epoch 9/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.7483 - accuracy: 0.4834\n","Epoch 9: loss improved from 1.78550 to 1.74832, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.7483 - accuracy: 0.4833\n","Epoch 10/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.7174 - accuracy: 0.4914\n","Epoch 10: loss improved from 1.74832 to 1.71743, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.7174 - accuracy: 0.4914\n","Epoch 11/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.6932 - accuracy: 0.4989\n","Epoch 11: loss improved from 1.71743 to 1.69321, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.6932 - accuracy: 0.4989\n","Epoch 12/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.6685 - accuracy: 0.5052\n","Epoch 12: loss improved from 1.69321 to 1.66852, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.6685 - accuracy: 0.5052\n","Epoch 13/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.6474 - accuracy: 0.5105\n","Epoch 13: loss improved from 1.66852 to 1.64743, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.6474 - accuracy: 0.5105\n","Epoch 14/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.6283 - accuracy: 0.5162\n","Epoch 14: loss improved from 1.64743 to 1.62832, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.6283 - accuracy: 0.5162\n","Epoch 15/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.6121 - accuracy: 0.5207\n","Epoch 15: loss improved from 1.62832 to 1.61206, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.6121 - accuracy: 0.5207\n","Epoch 16/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.5959 - accuracy: 0.5250\n","Epoch 16: loss improved from 1.61206 to 1.59586, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.5959 - accuracy: 0.5250\n","Epoch 17/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.5817 - accuracy: 0.5292\n","Epoch 17: loss improved from 1.59586 to 1.58166, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.5817 - accuracy: 0.5292\n","Epoch 18/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.5663 - accuracy: 0.5331\n","Epoch 18: loss improved from 1.58166 to 1.56632, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.5663 - accuracy: 0.5331\n","Epoch 19/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.5553 - accuracy: 0.5356\n","Epoch 19: loss improved from 1.56632 to 1.55536, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 55ms/step - loss: 1.5554 - accuracy: 0.5356\n","Epoch 20/20\n","1600/1601 [============================>.] - ETA: 0s - loss: 1.5448 - accuracy: 0.5392\n","Epoch 20: loss improved from 1.55536 to 1.54477, saving model to frankenstein_best_weights.h5\n","1601/1601 [==============================] - 89s 56ms/step - loss: 1.5448 - accuracy: 0.5392\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6320270110>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# pick a random seed\n","start = np.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","\n","result_string = \"\"\n","\n","# generate characters\n","for i in range(100):\n","  x = np.reshape(pattern, (1, len(pattern), 1))\n","  x = x / float(n_vocab)\n","  prediction = model.predict(x, verbose=0)\n","  index = np.argmax(prediction)\n","  result = int_to_char[index]\n","  result_string += result\n","  pattern.append(index)\n","  pattern = pattern[1:]\n","print(\"\\nDone.\")\n","\n","print(result_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0oMUyziBBHg","executionInfo":{"status":"ok","timestamp":1658770808836,"user_tz":300,"elapsed":4462,"user":{"displayName":"Octavio Navarro Hinojosa","userId":"04666797914243250800"}},"outputId":"c42d2323-1b7e-451a-e522-8706780a7bcd"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Seed:\n","\" air and revenge withdrew\n","\n","i left the room and locking the door made a solemn vow in my own\n","heart nev \"\n","\n","Done.\n","er were the soutow of the soot which i had been the soutow of the soirits of the soot which i had be\n"]}]}]}